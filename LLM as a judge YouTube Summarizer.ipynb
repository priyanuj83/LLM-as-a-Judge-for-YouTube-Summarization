{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d7099858-3f2e-46ce-8d28-96886c0abc4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sentence-transformers\n",
      "  Downloading sentence_transformers-5.1.0-py3-none-any.whl.metadata (16 kB)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in c:\\users\\priyanuj\\anaconda3\\lib\\site-packages (from sentence-transformers) (4.56.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\priyanuj\\anaconda3\\lib\\site-packages (from sentence-transformers) (4.66.5)\n",
      "Requirement already satisfied: torch>=1.11.0 in c:\\users\\priyanuj\\anaconda3\\lib\\site-packages (from sentence-transformers) (2.8.0+cpu)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\priyanuj\\anaconda3\\lib\\site-packages (from sentence-transformers) (1.5.1)\n",
      "Requirement already satisfied: scipy in c:\\users\\priyanuj\\anaconda3\\lib\\site-packages (from sentence-transformers) (1.13.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in c:\\users\\priyanuj\\anaconda3\\lib\\site-packages (from sentence-transformers) (0.34.4)\n",
      "Requirement already satisfied: Pillow in c:\\users\\priyanuj\\anaconda3\\lib\\site-packages (from sentence-transformers) (10.4.0)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in c:\\users\\priyanuj\\anaconda3\\lib\\site-packages (from sentence-transformers) (4.13.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\priyanuj\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\priyanuj\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2024.6.1)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\priyanuj\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\priyanuj\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.1)\n",
      "Requirement already satisfied: requests in c:\\users\\priyanuj\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.3)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\priyanuj\\anaconda3\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (1.13.3)\n",
      "Requirement already satisfied: networkx in c:\\users\\priyanuj\\anaconda3\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\priyanuj\\anaconda3\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.1.4)\n",
      "Requirement already satisfied: setuptools in c:\\users\\priyanuj\\anaconda3\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (75.1.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\priyanuj\\anaconda3\\lib\\site-packages (from tqdm->sentence-transformers) (0.4.6)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\priyanuj\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (1.26.4)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\priyanuj\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.9.11)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in c:\\users\\priyanuj\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.22.0)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\priyanuj\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.6.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\priyanuj\\anaconda3\\lib\\site-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\priyanuj\\anaconda3\\lib\\site-packages (from scikit-learn->sentence-transformers) (3.5.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\priyanuj\\anaconda3\\lib\\site-packages (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\priyanuj\\anaconda3\\lib\\site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\priyanuj\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\priyanuj\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\priyanuj\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\priyanuj\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2025.1.31)\n",
      "Downloading sentence_transformers-5.1.0-py3-none-any.whl (483 kB)\n",
      "Installing collected packages: sentence-transformers\n",
      "Successfully installed sentence-transformers-5.1.0\n"
     ]
    }
   ],
   "source": [
    "!pip install sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f49348bf-6c6c-4789-997c-1e66adb82c2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcript sample:\n",
      " When I was first learning to meditate, the instruction was to simply\n",
      "pay attention to my breath, and when my mind wandered,\n",
      "to bring it back. Sounded simple enough. Yet I'd sit on these silent retreats, sweating through T-shirts\n",
      "in the middle of winter. I'd take naps every chance I got\n",
      "because it was really hard work. Actually, it was exhausting. The instruction was simple enough but I was missing something\n",
      "really important. So why is it so hard to pay attention? Well, studies show that even whe\n"
     ]
    }
   ],
   "source": [
    "from youtube_transcript_api import YouTubeTranscriptApi\n",
    "\n",
    "# Extract video ID\n",
    "video_url = \"https://www.youtube.com/watch?v=-moW9jvvMr4\"\n",
    "video_id = video_url.split(\"v=\")[-1]\n",
    "\n",
    "# Create API object and fetch transcript\n",
    "ytt_api = YouTubeTranscriptApi()\n",
    "fetched_transcript = ytt_api.fetch(video_id)\n",
    "\n",
    "# Convert transcript to text\n",
    "transcript_text = \" \".join([snippet[\"text\"] for snippet in fetched_transcript.to_raw_data()])\n",
    "\n",
    "# Print first 500 characters\n",
    "print(\"Transcript sample:\\n\", transcript_text[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7dd9dcc1-2367-4351-9fd1-be5c29352d08",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "Device set to use cpu\n",
      "Some weights of PegasusForConditionalGeneration were not initialized from the model checkpoint at google/pegasus-xsum and are newly initialized: ['model.decoder.embed_positions.weight', 'model.encoder.embed_positions.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Device set to use cpu\n",
      "Both `max_new_tokens` (=256) and `max_length`(=150) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=150) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=150) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=150) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Candidate Summaries:\n",
      "\n",
      "BART:\n",
      "Even when we're really trying to pay attention to something, half of us drift off into a daydream. It turns out that we're fighting one of the most evolutionarily-conserved learning processes currently known in science. This reward-based learning process is called positive and negative reinforcement. In his lab, David Perry studied whether mindfulness training could help people quit smoking. He says the prefrontal cortex, that youngest part of our brain, understands on an intellectual level that we shouldn't smoke. Perry: \"What if instead we just got really curious about what was happening in our momentary experience?\" When the prefrontal cortex goes offline, we fall back into our old habits. Seeing what we get from our habits helps us understand them at a deeper level. In one study, mindfulness training was twice as good as gold standard therapy at helping people quit smoking. CNN's iReport.com will feature iReporter photos in a weekly Travel Snapshots gallery. Visit CNN.com/Travel each week for a new gallery of snapshots from around the world. This week, we look at the role of technology in helping people resist the urge to compulsively respond to texts.\n",
      "\n",
      "T5:\n",
      "we're fighting one of the most evolutionarily-conserved learning processes in science . this reward-based learning process is called positive and negative reinforcement . it's conserved back to the most basic nervous systems known to man . dr. sanjay gupta says mindfulness training can help people quit smoking and eating sweets . he says it taps into natural, reward-based learning process, but adds a twist . mindfulness training teaches us to be curious about what's happening in our momentary experience . mindfulness training is twice as good as gold standard therapy at helping people quit smoking . dr. sanjay gupta says mindfulness is about seeing clearly what we get from our habits . this disenchantment helps us understand them at a deeper level, he says . researchers are testing apps and online-based mindfulness training programs . they use the same technology that's driving us to distraction to help us quit habits . instead of see text message, compulsively text back, feel a little bit better .\n",
      "\n",
      "PEGASUS:\n",
      "In our series of letters from African-American journalists, film-maker and columnist Sharon Florentine looks at why it's so hard to pay attention to the world around us. You know, next time you feel bad, why don't you try eating something good so you'll feel better?\" We thank our brains for the great idea, try this and quickly learn that if we eat chocolate or ice cream when we're mad or sad, we feel better. When we're stressed out, we try to force ourselves to pay attention to our breath. Now, just like trying to force myself to pay attention to my breath, they could try to force themselves to quit smoking. She said, \"Mindful smoking: smells like stinky cheese and tastes like chemicals, YUCK!\" Now, she knew, cognitively that smoking was bad for her, that's why she joined our program. When the prefrontal cortex, the part of the brain that's responsible for decision-making, goes offline, it means that we're less interested in making decisions and more interested in making decisions that are helpful to us, such as not smoking. Here's the full text of my talk at the annual meeting of the American Association for the Advancement of Science, in which I explain why I'm testing apps and online-based mindfulness training programs that target these core mechanisms and, ironically, use the same technology that's driving us to distraction to help us step out of our unhealthy habit patterns of smoking, of stress eating and other addictive habits:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from summarizer_judge import load_summarizers, generate_summaries\n",
    "\n",
    "# Load models\n",
    "summarizers = load_summarizers()\n",
    "\n",
    "# Generate candidate summaries\n",
    "summaries = generate_summaries(summarizers, transcript_text, max_tokens=150)\n",
    "\n",
    "print(\"\\nCandidate Summaries:\\n\")\n",
    "for name, summary in summaries.items():\n",
    "    print(f\"{name.upper()}:\\n{summary}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e8d343a0-d98e-49b8-af26-d1b12f434d1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1bd9df2f989444568e648b32af030424",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Priyanuj\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Priyanuj\\.cache\\huggingface\\hub\\models--sentence-transformers--all-MiniLM-L6-v2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7d0011566a24d35bee3cfc427c03d12",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "facecc1f894142f38439ba4cde699320",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac14e0f1a07a4b278c9db3b536921715",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9314689bd92d47388f45a6a3e24aa3d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "859ff069ed6442e991d3b5a9de5f6e85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14a804dfdef0493b9e0c484a3c115c93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74ed2f548eb94ce9bb3f3e2cf0609dc8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10e2aa52454b43ac951419493a9ac1c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "431fcc39959c4a7bac54034693539c5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed7e1d7dfadc471cbd070e6e49ddab22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Judge Decision:\n",
      " Even when we're really trying to pay attention to something, half of us drift off into a daydream. It turns out that we're fighting one of the most evolutionarily-conserved learning processes currently known in science. This reward-based learning process is called positive and negative reinforcement. In his lab, David Perry studied whether mindfulness training could help people quit smoking. He says the prefrontal cortex, that youngest part of our brain, understands on an intellectual level that we shouldn't smoke. Perry: \"What if instead we just got really curious about what was happening in our momentary experience?\" When the prefrontal cortex goes offline, we fall back into our old habits. Seeing what we get from our habits helps us understand them at a deeper level. In one study, mindfulness training was twice as good as gold standard therapy at helping people quit smoking. CNN's iReport.com will feature iReporter photos in a weekly Travel Snapshots gallery. Visit CNN.com/Travel each week for a new gallery of snapshots from around the world. This week, we look at the role of technology in helping people resist the urge to compulsively respond to texts.\n"
     ]
    }
   ],
   "source": [
    "from judge import judge_summary\n",
    "\n",
    "decision = judge_summary(transcript_text, summaries)\n",
    "print(\"Judge Decision:\\n\", decision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ada497fb-0d06-4514-99ed-58651aeccfeb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
